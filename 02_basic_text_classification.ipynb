{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_basic_text_classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPZNIsDFh+56qGDycIXDjCV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiriuXProtocoL/Tensorflow_examples/blob/main/02_basic_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HppOeFt_uLS"
      },
      "source": [
        "#Sentiment analysis\n",
        "\n",
        "- train a model to predict movie reviews as positive or negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1oY3g2h_Mlg"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3XkUXGtAFlB"
      },
      "source": [
        "-  Large Movie Review Dataset that contains the text of 50,000 movie reviews from the Internet Movie Database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKU7DG7Y_ZU-"
      },
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiKn_82aAcxQ"
      },
      "source": [
        "- listing the downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxBNjmsNAMRt"
      },
      "source": [
        "os.listdir(dataset_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diErDik6AoOM"
      },
      "source": [
        "- openning the train folder and listing the contents\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VuCzuLDAfrm"
      },
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnds6CnJAu2-"
      },
      "source": [
        "- We are looking for the pos and neg folders\n",
        "- aclImdb/train/pos and aclImdb/train/neg directories\n",
        "- lets open a sample file inside positive folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q457wE9qAtUP"
      },
      "source": [
        "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJDG1HOPB4Sb"
      },
      "source": [
        "###Loading the dataset\n",
        "\n",
        "- We use the  text_dataset_from_directory utility, which expects a directory structure as follows:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "main_directory/\n",
        "...class_a/\n",
        "......a_text_1.txt\n",
        "......a_text_2.txt\n",
        "...class_b/\n",
        "......b_text_1.txt\n",
        "......b_text_2.txt\n",
        "```\n",
        "\n",
        "- We will have to remove all the other folders except pos and neg\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_iA9EylBque"
      },
      "source": [
        "#unsup,\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3gTh1bTC4Gc"
      },
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q66vqHnoDoj2"
      },
      "source": [
        "- leave the files as it is no problem\n",
        "- text_dataset_from_directory utility to create a labeled tf.data.Dataset\n",
        "- The IMDB dataset has already been divided into train and test, but it lacks a validation set.\n",
        "- Let's create a validation set using an 80:20 split of the training data by using the validation_split argument "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWLsUgRAC7RJ"
      },
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='training', \n",
        "    seed=seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBY1WGULFFWW"
      },
      "source": [
        "- So we have 25000 data in whole belonging to 2 different classes\n",
        "- We have split 80% as training i.e. 20000\n",
        "\n",
        "- Lets iterate over few exampls are print out some"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wtxqsbr4EEIW"
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(\"Review\", text_batch.numpy()[i])\n",
        "    print(\"Label\", label_batch.numpy()[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMJOotKQFsqQ"
      },
      "source": [
        "- We can see the reviews as well as their labels\n",
        "- lets review the labels again to identify the classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liXm64FJFcg9"
      },
      "source": [
        "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
        "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3zXlV7QGHaS"
      },
      "source": [
        "- create a validation and test dataset. \n",
        "- We will use the remaining 5,000 reviews from the training set for validation.\n",
        "- When using the validation_split and subset arguments, make sure to either specify a random seed, or to pass shuffle=False, so that the validation and training splits have no overlap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nZGqrKaGImI"
      },
      "source": [
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='validation', \n",
        "    seed=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzjO5HluGR6U"
      },
      "source": [
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test', \n",
        "    batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIbE-fr1GcL7"
      },
      "source": [
        "###Prepare the dataset for training\n",
        "- Next, We will standardize, tokenize, and vectorize the data using the helpful preprocessing.TextVectorization layer. \n",
        "\n",
        "**Standardization**\n",
        "- As you saw above, the reviews contain various HTML tags like \"< br >tags\"\n",
        "\n",
        "- These tags will not be removed by the default standardizer in the TextVectorization layer (which converts text to lowecase and strips punctuation by default, but doesn't strip HTML). \n",
        "- We will write a custom standardization function to remove the HTML.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rtF1IRlGXLe"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwCJhaykPalZ"
      },
      "source": [
        "- TextVectorization layer. we will use this layer to standardize, tokenize, and vectorize our data. \n",
        "- We set the output_mode to int to create unique integer indices for each token.\n",
        "- We'll also define some constants for the model, like an explicit maximum sequence_length, which will cause the layer to pad or truncate sequences to exactly sequence_length values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZMhTjRiO99O"
      },
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCfdzxnoP_Ds"
      },
      "source": [
        "- We will call adapt to fit the state of the preprocessing layer to the dataset. This will cause the model to build an index of strings to integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLOoThhGPik2"
      },
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS0-0Z2hQCVU"
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jliWEo-fQM8E"
      },
      "source": [
        "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review\", first_review)\n",
        "print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized review\", vectorize_text(first_review, first_label))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKgiiRDHQdaK"
      },
      "source": [
        "- As you can see above, each token has been replaced by an integer. You can lookup the token (string) that each integer corresponds to by calling .get_vocabulary() on the layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IB9K0lRQO7Z"
      },
      "source": [
        "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
        "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9jIk6IbQrRO"
      },
      "source": [
        "- We will apply the TextVectorization layer you created earlier to the train, validation, and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuf70MlvQgIE"
      },
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbkgegt5Q4Y5"
      },
      "source": [
        "###Configure the dataset for performance\n",
        "- These are two important methods you should use when loading data to make sure that I/O does not become blocking.\n",
        "- .cache() keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training your model.\n",
        "- If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\n",
        "\n",
        "- .prefetch() overlaps data preprocessing and model execution while training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o82ud9JrQvK-"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTmlP3wzRbFH"
      },
      "source": [
        "###Create the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfqNQSxeRWqm"
      },
      "source": [
        "embedding_dim = 16\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(1)])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koEG3siJR-5K"
      },
      "source": [
        "- The layers are stacked sequentially to build the classifier:\n",
        "\n",
        "    - The first layer is an Embedding layer. This layer takes the integer-encoded reviews and looks up an embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding).\n",
        "    - Next, a GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n",
        "    - This fixed-length output vector is piped through a fully-connected (Dense) layer with 16 hidden units.\n",
        "    - The last layer is densely connected with a single output node.\n",
        "\n",
        "###Loss function and optimizer\n",
        "\n",
        "- A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), you'll use losses.BinaryCrossentropy loss function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SLlnx25Rg8J"
      },
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoJPEo8sS4vd"
      },
      "source": [
        "###Train the model\n",
        "- You will train the model by passing the dataset object to the fit method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUjGhlzmS2xv"
      },
      "source": [
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUAKqH5QTE3W"
      },
      "source": [
        "###Evaluate the model\n",
        "\n",
        "- Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIUZ31BlTCGo"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNXzS9RxTPC_"
      },
      "source": [
        "- This fairly naive approach achieves an accuracy of about 86%\n",
        "###Create a plot of accuracy and loss over time\n",
        "\n",
        "- model.fit() returns a History object that contains a dictionary with everything that happened during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTCjxE-eTP0Q"
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE0wLyquThY0"
      },
      "source": [
        "- There are four entries: one for each monitored metric during training and validation. \n",
        "- We can use these to plot the training and validation loss for comparison, as well as the training and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYY5kCXRTjWK"
      },
      "source": [
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prOnOAuyTldL"
      },
      "source": [
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnZ6uC02UXUj"
      },
      "source": [
        "- the training loss decreases with each epoch and the training accuracy increases with each epoch. \n",
        "- This is expected when using a gradient descent optimization—it should minimize the desired quantity on every iteration.\n",
        "- validation loss and accuracy—they seem to peak before the training accuracy. \n",
        "- This is an example of overfitting: the model performs better on the training data than it does on data it has never seen before. After this point, the model over-optimizes and learns representations specific to the training data that do not generalize to test data.\n",
        "\n",
        "- For this particular case, you could prevent overfitting by simply stopping the training when the validation accuracy is no longer increasing. One way to do so is to use the EarlyStopping callback.\n",
        "\n",
        "###Export the model\n",
        "\n",
        "-  If you want to make your model capable of processing raw strings (for example, to simplify deploying it), you can include the TextVectorization layer inside your model. To do so, you can create a new model using the weights you just trained.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tXCV0giTqql"
      },
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "  layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFDsZxXhVMuG"
      },
      "source": [
        "#Inference on new data\n",
        "\n",
        "- To get predictions for new examples, you can simply call model.predict()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVmZXkEnVJxo"
      },
      "source": [
        "examples = [\n",
        "  \"The movie was great!\",\n",
        "  \"The movie was okay.\",\n",
        "  \"The movie was terrible...\"\n",
        "]\n",
        "\n",
        "export_model.predict(examples)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr_rBUtiVqa2"
      },
      "source": [
        "- Superb We have made our predictions correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC-RrSDoVvMN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}