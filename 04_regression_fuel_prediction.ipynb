{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_regression_fuel_prediction.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMH2hUsRtzTdYVLWDwW6H9I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiriuXProtocoL/Tensorflow_examples/blob/main/04_regression_fuel_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFle5OKk99L1"
      },
      "source": [
        "###Basic regression: Predict fuel efficiency\n",
        "- We will use the Auto MPG Dataset from the UCI Machine Learning Repository and builds a model to predict the fuel efficiency of late-1970s and early 1980s automobile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY7WfN2Q9eQa"
      },
      "source": [
        "# Use seaborn for pairplot\n",
        "!pip install -q seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb-VqabQ-K0O"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Make numpy printouts easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH6Y146W-RZt"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRy-Rn-J-fqT"
      },
      "source": [
        "###Load the Data\n",
        "- First download and import the dataset using pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqDbFESj-TzO"
      },
      "source": [
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "\n",
        "raw_dataset = pd.read_csv(url, names=column_names,\n",
        "                          na_values='?', comment='\\t',\n",
        "                          sep=' ', skipinitialspace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfzNBU5j_iEg"
      },
      "source": [
        "dataset = raw_dataset.copy()\n",
        "dataset.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpxI7Z4N_yQ6"
      },
      "source": [
        "###Clean the data\n",
        "- Check for null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAq7KJor_qkF"
      },
      "source": [
        "dataset.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqLnkixg_9Hk"
      },
      "source": [
        "- Lets just drop those rows in Horsepower\n",
        "- othe methods like mean, median, mode replacements can also be adopted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ydqEq0fMo8h"
      },
      "source": [
        "dataset = dataset.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZCKvdyHANcy"
      },
      "source": [
        "- The Origin column is a categorical column not a numeric one so we will have to convert that using the one hot encoding vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4xkyPlA_4iE"
      },
      "source": [
        "dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2Y1BzbqAazt"
      },
      "source": [
        "dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
        "dataset.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og4hJaoYAlh1"
      },
      "source": [
        "###Split the data into train and test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWE-R2mOAgpV"
      },
      "source": [
        "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CwReGxKA5e6"
      },
      "source": [
        "###Inspect the data\n",
        "- Have a quick look at the joint distribution of a few pairs of columns from the training set.\n",
        "\n",
        "- Looking at the top row it should be clear that the fuel efficiency (MPG) is a function of all the other parameters. Looking at the other rows it should be clear that they are each functions of eachother."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOUmxGGlAud7"
      },
      "source": [
        "sns.pairplot(train_dataset[['MPG', 'Cylinders', 'Displacement', 'Weight']], diag_kind='kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBXrsayuBVYo"
      },
      "source": [
        "train_dataset.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoyuiUkxBlXa"
      },
      "source": [
        "###Split features from labels\n",
        "- Separate the target value, the \"label\", from the features. This label is the value that you will train the model to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iuO5o4fBaqf"
      },
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "\n",
        "train_labels = train_features.pop('MPG')\n",
        "test_labels = test_features.pop('MPG')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSkONqTnB2JK"
      },
      "source": [
        "###Normalization\n",
        "- It is good practice to normalize features that use different scales and ranges.\n",
        "\n",
        "- One reason this is important is because the features are multiplied by the model weights. So the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\n",
        "\n",
        "- Although a model might converge without feature normalization, normalization makes training much more stable. \n",
        "\n",
        "- We can observe that all the values are at different scales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb0wyI78BxZL"
      },
      "source": [
        "train_dataset.describe().transpose()[['mean', 'std']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFSkkiP0CRNR"
      },
      "source": [
        "- The preprocessing.Normalization layer is a clean and simple way to build that preprocessing into your model.\n",
        "\n",
        "- The first step is to create the layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO8_YeY4B5Sv"
      },
      "source": [
        "normalizer = preprocessing.Normalization()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubj5uf8XCWKN"
      },
      "source": [
        "- Then .adapt() it to the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ryuLkSuCUcX"
      },
      "source": [
        "normalizer.adapt(np.array(train_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgIn4hyyCeCt"
      },
      "source": [
        "- This calculates the mean and variance, and stores them in the layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk4KmQ_RCY7W"
      },
      "source": [
        "print(normalizer.mean.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R6TNP3QCgWF"
      },
      "source": [
        "- When the layer is called it returns the input data, with each feature independently normalized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9QyVn4kCfqN"
      },
      "source": [
        "first = np.array(train_features[:1])\n",
        "\n",
        "with np.printoptions(precision=2, suppress=True):\n",
        "  print('First example:', first)\n",
        "  print()\n",
        "  print('Normalized:', normalizer(first).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFKglhpUK02V"
      },
      "source": [
        "###Linear regression\n",
        "**One Variable**\n",
        "- A single-variable linear regression, to predict MPG from Horsepower\n",
        "  - Normalize the input horsepower.\n",
        "  - Apply a linear transformation (y = mx + c) to produce 1 output using layers.Dense\n",
        "- The number of inputs can either be set by the input_shape argument, or automatically when the model is run for the first time.\n",
        "\n",
        "- First create the horsepower Normalization layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWEXNb2qClmq"
      },
      "source": [
        "horsepower = np.array(train_features['Horsepower'])\n",
        "\n",
        "horsepower_normalizer = preprocessing.Normalization(input_shape=[1,])\n",
        "horsepower_normalizer.adapt(horsepower)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bojhY7ULcG1"
      },
      "source": [
        "- Build the sequential model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XMwAVRNLaOB"
      },
      "source": [
        "horsepower_model = tf.keras.Sequential([\n",
        "    horsepower_normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "horsepower_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E3Vh3AELorv"
      },
      "source": [
        "- This model will predict MPG from Horsepower.\n",
        "\n",
        "- Run the untrained model on the first 10 horse-power values. The output won't be good, but you'll see that it has the expected shape, (10,1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI2Y__m9LhhI"
      },
      "source": [
        "horsepower_model.predict(horsepower[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsxOE95cNEdv"
      },
      "source": [
        "- Once the model is built, configure the training procedure using the Model.compile() method. \n",
        "- The most important arguments to compile are the loss and the optimizer since these define what will be optimized (mean_absolute_error) and how (using the optimizers.Adam)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLVAUTe4Lu_s"
      },
      "source": [
        "horsepower_model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pAzNwxpNKyO"
      },
      "source": [
        "- Once the training is configured, use Model.fit() to execute the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ6dojJaNHm3"
      },
      "source": [
        "%%time\n",
        "history = horsepower_model.fit(\n",
        "    train_features['Horsepower'], train_labels,\n",
        "    epochs=100,\n",
        "    # suppress logging\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data\n",
        "    validation_split = 0.2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLdduOArNTJD"
      },
      "source": [
        "- Visualize the model's training progress using the stats stored in the history object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlOLbiaINM6L"
      },
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z-TiwNeNi6K"
      },
      "source": [
        "- Let have a plot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJoa0TVeNXHA"
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.ylim([0, 10])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error [MPG]')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJmwuJLUNkq6"
      },
      "source": [
        "plot_loss(history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VckxkoAHNuxZ"
      },
      "source": [
        "- Collect the results on the test set, for later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HOpISc9No8t"
      },
      "source": [
        "test_results = {}\n",
        "\n",
        "test_results['horsepower_model'] = horsepower_model.evaluate(\n",
        "    test_features['Horsepower'],\n",
        "    test_labels, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nsGqlcIN4hL"
      },
      "source": [
        "- SInce this is a single variable regression it's easy to look at the model's predictions as a function of the input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVFf2M-kNz99"
      },
      "source": [
        "x = tf.linspace(0.0, 250, 251)\n",
        "y = horsepower_model.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KhVHgNGN8m-"
      },
      "source": [
        "def plot_horsepower(x, y):\n",
        "  plt.scatter(train_features['Horsepower'], train_labels, label='Data')\n",
        "  plt.plot(x, y, color='k', label='Predictions')\n",
        "  plt.xlabel('Horsepower')\n",
        "  plt.ylabel('MPG')\n",
        "  plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnkyFhI9OAGH"
      },
      "source": [
        "plot_horsepower(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPPSoMwPOLg3"
      },
      "source": [
        "**Multiple inputs**\n",
        "- We can use an almost identical setup (y=mx + c) to make predictions based on multiple inputs. This model still does the same except that m is a matrix and c is a vector.\n",
        "\n",
        "- This time use the Normalization layer that was adapted to the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSiedsH5OCBS"
      },
      "source": [
        "linear_model = tf.keras.Sequential([\n",
        "    normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTmp-jCpOh8r"
      },
      "source": [
        "- When you call this model on a batch of inputs, it produces units=1 outputs for each example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpOq4XofOeWa"
      },
      "source": [
        "linear_model.predict(train_features[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY3TZltrOsmv"
      },
      "source": [
        "- When you call the model it's weight matrices will be built. Now you can see that the kernel (the m in y = mx + c) has a shape of (9,1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9AQIY-NOmIC"
      },
      "source": [
        "linear_model.layers[1].kernel\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1UjDWTKPNUq"
      },
      "source": [
        "- Use the same compile and fit calls as for the single input horsepower model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siBixNUuPHxz"
      },
      "source": [
        "linear_model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQwIY9rLPQKk"
      },
      "source": [
        "%%time\n",
        "history = linear_model.fit(\n",
        "    train_features, train_labels, \n",
        "    epochs=100,\n",
        "    # suppress logging\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data\n",
        "    validation_split = 0.2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoLNmtfKPWKQ"
      },
      "source": [
        "- Using all the inputs achieves a much lower training and validation error than the horsepower model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XludSXqoPSWp"
      },
      "source": [
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNyry52qPhWS"
      },
      "source": [
        "- Collect the results on the test set, for later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZmGatJrPaAr"
      },
      "source": [
        "test_results['linear_model'] = linear_model.evaluate(\n",
        "    test_features, test_labels, verbose=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzh1R3iVPmsL"
      },
      "source": [
        "###A DNN regression\n",
        "\n",
        "> The code is basically the same except the model is expanded to include some \"hidden\" non-linear layers.\n",
        "\n",
        "- These models will contain a few more layers than the linear model:\n",
        "\n",
        "    - The normalization layer.\n",
        "    - Two hidden, nonlinear, Dense layers using the relu nonlinearity.\n",
        "    - A linear single-output layer.\n",
        "\n",
        "- Both will use the same training procedure so the compile method is included in the build_and_compile_model function below.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjslq2WKPjmP"
      },
      "source": [
        "def build_and_compile_model(norm):\n",
        "  model = keras.Sequential([\n",
        "      norm,\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  model.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHRSZ1M9QE9O"
      },
      "source": [
        "**One variable**\n",
        "- Start with a DNN model for a single input: \"Horsepower\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk21ty3LQAMH"
      },
      "source": [
        "dnn_horsepower_model = build_and_compile_model(horsepower_normalizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAFgjoGoQNGg"
      },
      "source": [
        "- This model has quite a few more trainable parameters than the linear models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p-OlN5HQKmb"
      },
      "source": [
        "dnn_horsepower_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTaOHijfQTsi"
      },
      "source": [
        "- Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bgVHoO6QP3d"
      },
      "source": [
        "%%time\n",
        "history = dnn_horsepower_model.fit(\n",
        "    train_features['Horsepower'], train_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=0, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCNREewwQaW_"
      },
      "source": [
        "- This model does slightly better than the linear-horsepower model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfBe2BYbQWIk"
      },
      "source": [
        "plot_loss(history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AADWOmqTQjfs"
      },
      "source": [
        "- If you plot the predictions as a function of Horsepower, you'll see how this model takes advantage of the nonlinearity provided by the hidden layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYbiI6XnQdcL"
      },
      "source": [
        "x = tf.linspace(0.0, 250, 251)\n",
        "y = dnn_horsepower_model.predict(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8sN-z_OQmGA"
      },
      "source": [
        "plot_horsepower(x, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD7cwRbuQrYJ"
      },
      "source": [
        "- Collect the results on the test set, for later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOgCdT2PQnqV"
      },
      "source": [
        "test_results['dnn_horsepower_model'] = dnn_horsepower_model.evaluate(\n",
        "    test_features['Horsepower'], test_labels,\n",
        "    verbose=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEOI4-5cQx9I"
      },
      "source": [
        "###Full Model\n",
        "**Multiple variable**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFsWoPddQtdp"
      },
      "source": [
        "dnn_model = build_and_compile_model(normalizer)\n",
        "dnn_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiKmz3m4Q6KR"
      },
      "source": [
        "%%time\n",
        "history = dnn_model.fit(\n",
        "    train_features, train_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=0, epochs=100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCWGRrnvQ8od"
      },
      "source": [
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shdhh-zmRCsD"
      },
      "source": [
        "- Collect the results on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxvqYs7oQ_ej"
      },
      "source": [
        "test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQk0MfssRI4q"
      },
      "source": [
        "###Performance\n",
        "- Now that all the models are trained check the test-set performance and see how they did"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kXarK0SRGdh"
      },
      "source": [
        "pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8j4rbMiRftJ"
      },
      "source": [
        "- These results match the validation error seen during training\n",
        "###MAke Predictions\n",
        "- Finally, predict have a look at the errors made by the model when making predictions on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkNzn-MEROJC"
      },
      "source": [
        "test_predictions = dnn_model.predict(test_features).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [MPG]')\n",
        "plt.ylabel('Predictions [MPG]')\n",
        "lims = [0, 50]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa1l2NOIRuWA"
      },
      "source": [
        "- It looks like the model predicts reasonably well.\n",
        "\n",
        "- Now take a look at the error distribution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WTMWFriRrTe"
      },
      "source": [
        "error = test_predictions - test_labels\n",
        "plt.hist(error, bins=25)\n",
        "plt.xlabel('Prediction Error [MPG]')\n",
        "_ = plt.ylabel('Count')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwOcyB09R1_0"
      },
      "source": [
        "- If you're happy with the model save it for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V47OV1ER3DR"
      },
      "source": [
        "dnn_model.save('dnn_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA2oULaAR7Ux"
      },
      "source": [
        "- If you reload the model, it gives identical output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvtDtRu_R8Lj"
      },
      "source": [
        "reloaded = tf.keras.models.load_model('dnn_model')\n",
        "\n",
        "test_results['reloaded'] = reloaded.evaluate(\n",
        "    test_features, test_labels, verbose=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6gPAUT1SPm3"
      },
      "source": [
        "pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92ebVruXSVtr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}