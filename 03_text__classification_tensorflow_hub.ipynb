{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_text _classification_tensorflow_hub.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "15_H-HiVPdjscLxK6qrlandC0-0f4jj3e",
      "authorship_tag": "ABX9TyN68VMPM7VdsT8XbL+Ysffp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiriuXProtocoL/Tensorflow_examples/blob/main/03_text__classification_tensorflow_hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUcyDC9Pwhse"
      },
      "source": [
        "###Movie Sentiment Analysis using tensorflow Hub\n",
        "- This demonstrates the basic application of transfer learning with TensorFlow Hub and Keras.\n",
        "- We'll use the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database.\n",
        "-  tensorflow_hub, a library for loading trained models from TFHub in a single line of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgu1qDIIwQGx"
      },
      "source": [
        "!pip install -q tfds-nightly\n",
        "!pip install -q tensorflow-hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_o6okLwxCtH"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load compressed models from tensorflow_hub\n",
        "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"COMPRESSED\"\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQCrdo9Vx65d"
      },
      "source": [
        "###Download the IMDB dataset\n",
        "- The IMDB dataset is available on imdb reviews or on TensorFlow datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgVUezgExNOK"
      },
      "source": [
        "# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\n",
        "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
        "# storing to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.\n",
        "train_data, validation_data, test_data = tfds.load(\n",
        "    name=\"imdb_reviews\", \n",
        "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
        "    as_supervised=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAK5dkGdypmo"
      },
      "source": [
        "###Explore the data\n",
        "- printing first 10 examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Xdk5gtyHXY"
      },
      "source": [
        "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
        "train_examples_batch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlk6Z4YCy-Qv"
      },
      "source": [
        "- printing first 10 labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5JtoYRPy9K8"
      },
      "source": [
        "train_labels_batch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KC4Nmn3zHkT"
      },
      "source": [
        "###Build the model\n",
        "- We can use a pre-trained text embedding as the first layer, which will have three advantages:\n",
        "\n",
        "    - we don't have to worry about text preprocessing,\n",
        "    - we can benefit from transfer learning, the embedding has a fixed size, so it's simpler to process.\n",
        "\n",
        "- For this example we will use a pre-trained text embedding model from TensorFlow Hub called google/nnlm-en-dim50/2.\n",
        "\n",
        "- there are a lot of text embedding models avaliable in tensorflow hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejk20fYLzCJE"
      },
      "source": [
        "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
        "                           dtype=tf.string, trainable=True)\n",
        "hub_layer(train_examples_batch[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pf2C6zI3FAs"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOjsuzNh3h6L"
      },
      "source": [
        "- The layers are stacked sequentially to build the classifier:\n",
        "\n",
        "    - The first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The pre-trained text embedding model that we are using (google/nnlm-en-dim50/2) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: (num_examples, embedding_dimension). For this NNLM model, the embedding_dimension is 50.\n",
        "    - This fixed-length output vector is piped through a fully-connected (Dense) layer with 16 hidden units.\n",
        "    - The last layer is densely connected with a single output node.\n",
        "\n",
        "###Loss function and optimizer\n",
        "\n",
        "- Since this is a binary classification problem and the model outputs logits (a single-unit layer with a linear activation), we'll use the binary_crossentropy loss function.\n",
        "- it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nT2NZJ-3ID8"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjFl_mH14Fhv"
      },
      "source": [
        "###Train the model\n",
        "- Train the model for 10 epochs in mini-batches of 512 samples. \n",
        "- This is 10 iterations over all samples in the x_train and y_train tensors.\n",
        "- While training, monitor the model's loss and accuracy on the 10,000 samples from the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BPMbiRw4CME"
      },
      "source": [
        "history = model.fit(train_data.shuffle(10000).batch(512),\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_data.batch(512),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXljR5el4ZPu"
      },
      "source": [
        "###Evaluate the model\n",
        "- Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypVXW35U4UEO"
      },
      "source": [
        "results = model.evaluate(test_data.batch(512), verbose=2)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-cT3-914lLY"
      },
      "source": [
        "- This fairly naive approach achieves an accuracy of about 85%. \n",
        "- With more advanced approaches, the model should get closer to 95%.\n",
        "- Lets try another model from tensorflow hub and see what happens to the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIIwyft84gRM"
      },
      "source": [
        "embedding = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
        "                           dtype=tf.string, trainable=True)\n",
        "hub_layer(train_examples_batch[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W6JuOwv4y6w"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezlY0gh844hS"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhf55Mmr45k2"
      },
      "source": [
        "history = model.fit(train_data.shuffle(10000).batch(512),\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_data.batch(512),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6loOxwK49nk"
      },
      "source": [
        "results = model.evaluate(test_data.batch(512), verbose=2)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imnWy-2L7xNx"
      },
      "source": [
        "YEss we got an increased accuracy of 87.6%"
      ]
    }
  ]
}